#!/usr/bin/env python3
import os
import re
import json
import logging
import asyncio
import time
import os
import aiofiles
from typing import Dict, Any
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from editing.code_opti.set_metric import evaluate_svg_optimization
from eval_util import setup_logger,extract_svg_from_response
from openai import AsyncOpenAI

logger=setup_logger(name="svg_optimization", log_dir="../logs", log_filename="svg_optimization.log")

API_KEY = "mock-key-123"  
BASE_URL = "http://localhost:8000/v1"
AVAILABLE_MODELS = ["Qwen2-72B-Instruct-AWQ", "gpt-4o", "deepseekr1","mock-llm"]

async def optimize_svg_from_api(origin_svg: str, target_ratio: float = None, model: str = "deepseekr1", semaphore: asyncio.Semaphore = None) -> Tuple[Optional[str], float, Optional[str]]:
    """
    Optimize SVG using API
    
    """
    start_time = time.time()
    
    system_prompt = (
        '''You are an advanced SVG optimization expert, skilled at maximizing compression and optimization of SVG code while maintaining visual consistency and ensuring code correctness.
        Please optimize the user-provided SVG code according to the following core principles:
            1. Remove metadata and editor information
            - Clear metadata, comments, and unnecessary attributes generated by design software
            - Remove hidden elements and empty tags

            2. Path optimization
            - Simplify path data, reduce control points
            - Lower decimal precision (1-2 places is usually sufficient)
            - Merge similar paths

            3. Attribute and style processing
            - Remove redundant or default attribute values
            - Merge duplicate styles
            - Optimize color representation (e.g., #000 instead of #000000)

            4. Structure optimization
            - Remove unnecessary grouping and nesting
            - Optimize IDs and class names
            - Ensure viewBox is set correctly

            5. Compression and fine-tuning
            - Remove unnecessary whitespace and units
            - Use short commands instead of long format commands

            After optimization, please strictly return the complete optimized code in the following format:Answer: {SVG code},Provide the complete optimized code only in the {SVG code} position, without adding any explanations, comments, or other content.
            '''
    )
    
    user_prompt = f"""Here is the original SVG to optimize:{origin_svg}."""
    aclient = AsyncOpenAI(
        base_url=BASE_URL,
        api_key=API_KEY,
    )
    
    async def execute_api_call():
        try:
            retries = 0
            max_retries = 10
            
            while retries < max_retries:
                try:
                    stream = await aclient.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        stream=True,
                    )
                    response = ""
                    async for chunk in stream:
                        if chunk.choices[0].delta.content:
                            content = chunk.choices[0].delta.content
                            response += content  

                    execution_time = time.time() - start_time
                    
                    response_text = response
                    svg_code = extract_svg_from_response(response_text)
                    
                    return svg_code, execution_time, response_text
                    
                except Exception as e:
                    retries += 1
                    logger.error(f"API call failed: {e}, retry {retries}")
                    if retries >= max_retries:
                        logger.error(f"Exceeded maximum retries, returning error")
                        return None, time.time() - start_time, "request error"
                    await asyncio.sleep(10) 
        
        except Exception as e:
            logger.error(f"Exception during API call: {e}")
            return None, time.time() - start_time, None
    
    if semaphore:
        async with semaphore:
            return await execute_api_call()
    else:

        return await execute_api_call()

async def process_single_optimization_sample(sample_id: str, sample_data: Dict[str, Any], model: str, output_file: str, semaphore: asyncio.Semaphore = None) -> Dict[str, Any]:
    """
    Process a single SVG optimization sample and save original SVG, optimized SVG, 
    and target optimized SVG (if exists) to subfolders.
    """
    origin_svg = sample_data.get("origin_svg", "")
    target_opti_svg = sample_data.get("opti_svg", "")
    target_opti_ratio = sample_data.get("opti_ratio")
    
    svg_path = sample_data.get("svg_path", "")
    svg_filename = os.path.basename(svg_path) if svg_path else f"sample_{sample_id}"

    if not origin_svg:
        return {
            "sample_id": sample_id,
            "error": "Missing origin_svg field"
        }

    logger.info(f"Processing sample {sample_id} ({svg_filename})...")

   
    optimized_svg, execution_time, full_response = await optimize_svg_from_api(
        origin_svg, 
        target_opti_ratio, 
        model, 
        semaphore
    )

    if not optimized_svg:
        return {
            "sample_id": sample_id,
            "svg_filename": svg_filename,
            "error": "SVG optimization failed",
            "execution_time": execution_time,
            "full_response": full_response,
            "origin_svg": origin_svg,
            "target_opti_svg": target_opti_svg,
            "target_opti_ratio": target_opti_ratio
        }

   
    save_dir = os.path.join(os.path.dirname(output_file), svg_filename)
    os.makedirs(save_dir, exist_ok=True)

    origin_path = os.path.join(save_dir, "origin.svg")
    gen_path = os.path.join(save_dir, "gen.svg")
    target_path = os.path.join(save_dir, "target.svg")

    async with aiofiles.open(origin_path, 'w', encoding='utf-8') as f:
        await f.write(origin_svg)

    async with aiofiles.open(gen_path, 'w', encoding='utf-8') as f:
        await f.write(optimized_svg)

    if target_opti_svg:
        async with aiofiles.open(target_path, 'w', encoding='utf-8') as f:
            await f.write(target_opti_svg)

    # Evaluate optimization result
    evaluation_result = None
    if optimized_svg:
        evaluation_result = evaluate_svg_optimization(
            ori_svg=origin_svg,
            gen_svg=optimized_svg,
            opti_ratio=target_opti_ratio,
            execution_time=execution_time
        )

    result = {
        "sample_id": sample_id,
        "svg_filename": svg_filename,
        "origin_svg": origin_svg,
        "optimized_svg": optimized_svg,
        "target_opti_svg": target_opti_svg,
        "target_opti_ratio": target_opti_ratio,
        "execution_time": execution_time,
        "full_response": full_response
    }

    if evaluation_result:
        result["evaluation"] = evaluation_result

    async with aiofiles.open(output_file, 'a', encoding='utf-8') as f:
        import json
        await f.write(json.dumps(result, ensure_ascii=False) + '\n')

    return result

async def process_optimization_batch_from_json(json_file: str, output_file: str, model: str = "deepseekr1", max_concurrent: int = 5) -> Dict[str, Any]:
    """
    Batch process SVG optimization samples from JSON file
    """
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            samples_data = json.load(f)
            
        if not isinstance(samples_data, list):
            logger.error(f"JSON data must be in list format")
            return {"error": "Invalid JSON data format, should be list"}
            
        samples = samples_data
    except Exception as e:
        logger.error(f"Failed to load JSON file {json_file}: {e}")
        return {"error": f"Failed to load JSON file: {e}"}
    
    if not samples:
        return {"error": "Sample data is empty"}

    output_dir = os.path.dirname(os.path.abspath(output_file))
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    sample_ids = [f"sample_{i+1}" for i in range(len(samples))]

    semaphore = asyncio.Semaphore(max_concurrent)

    results = []
    total_samples = len(samples)
    processed = 0

    batch_size = max_concurrent * 2  
    for start_idx in range(0, total_samples, batch_size):
        end_idx = min(start_idx + batch_size, total_samples)
        batch_ids = sample_ids[start_idx:end_idx]
        batch_samples = samples[start_idx:end_idx]
        
        logger.info(f"Processing batch {start_idx//batch_size + 1}, samples {start_idx+1}-{end_idx}...")
        
        batch_tasks = [
            process_single_optimization_sample(sample_id, sample, model, output_file, semaphore)
            for sample_id, sample in zip(batch_ids, batch_samples)
        ]
        
        batch_results = await asyncio.gather(*batch_tasks)
        results.extend(batch_results)
        processed += len(batch_results)
        
        interim_results = {
            result["sample_id"]: result
            for result in results
            if "sample_id" in result
        }
        
        interim_output = f"{os.path.splitext(output_file)[0]}_interim_{processed}.json"
        with open(interim_output, 'w', encoding='utf-8') as f:
            json.dump(interim_results, f, indent=2, ensure_ascii=False)
        logger.info(f"Processed {processed}/{total_samples} samples, interim results saved to: {interim_output}")
    
    detailed_results = {}
    for result in results:
        if "sample_id" in result:
            detailed_results[result["sample_id"]] = result
    
    successful_samples = [r for r in results if "error" not in r]
    sample_count = len(successful_samples)

    total_execution_time = 0
    total_compression_ratio = 0
    total_size_reduction = 0
    total_target_difference = 0
    total_mse = 0
   
    zero_mse_count = 0
    zero_mse_compression_ratio = 0
    zero_mse_size_reduction = 0
    
    target_count = 0
    target_difference_count = 0
    mse_count = 0
    
    evaluation_count = 0
    
    for result in successful_samples:
        total_execution_time += result["execution_time"]
        
        if "evaluation" in result:
            eval_data = result["evaluation"]
            evaluation_count += 1
            
            total_compression_ratio += eval_data["compression"]["ratio_percent"]
            total_size_reduction += eval_data["compression"]["size_reduction_percent"]
            
            if "target_difference" in eval_data["compression"]:
                total_target_difference += eval_data["compression"]["target_difference"]
                target_difference_count += 1

            if "mse" in eval_data:
                current_mse = eval_data["mse"]
                total_mse += current_mse
                mse_count += 1

                if current_mse == 0:
                    zero_mse_count += 1
                    zero_mse_compression_ratio += eval_data["compression"]["ratio_percent"]
                    zero_mse_size_reduction += eval_data["compression"]["size_reduction_percent"]
        
        if "target_opti_ratio" in result and result["target_opti_ratio"] is not None:
            target_count += 1

    summary = {
        "total_samples": len(samples),
        "successful_samples": sample_count,
        "samples_evaluated": evaluation_count,
        "detailed_results": detailed_results,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    if sample_count > 0:
        summary["average_execution_time"] = total_execution_time / sample_count

    if evaluation_count > 0:
        avg_evaluation = {
            "average_compression": {
                "ratio_percent": total_compression_ratio / evaluation_count,
                "size_reduction_percent": total_size_reduction / evaluation_count
            }
        }

        if mse_count > 0:
            avg_evaluation["average_mse"] = total_mse / mse_count
            avg_evaluation["mse_zero_count"] = zero_mse_count
            avg_evaluation["mse_zero_percentage"] = zero_mse_count / mse_count * 100 if mse_count > 0 else 0
 
            if zero_mse_count > 0:
                avg_evaluation["zero_mse_samples"] = {
                    "count": zero_mse_count,
                    "average_compression_ratio": zero_mse_compression_ratio / zero_mse_count,
                    "average_size_reduction": zero_mse_size_reduction / zero_mse_count
                }

        if target_difference_count > 0:
            avg_evaluation["average_compression"]["target_difference"] = total_target_difference / target_difference_count
            avg_evaluation["samples_with_target_ratio"] = target_count
        
        summary["evaluation_summary"] = avg_evaluation

    if evaluation_count > 0:
        final_avg_metrics = {
            "final_average_metrics": {
                "execution_time_seconds": total_execution_time / sample_count if sample_count > 0 else 0,
                "compression_ratio_percent": total_compression_ratio / evaluation_count,
                "size_reduction_percent": total_size_reduction / evaluation_count
            },
            "success_rate": sample_count / len(samples) if len(samples) > 0 else 0,
            "evaluation_rate": evaluation_count / len(samples) if len(samples) > 0 else 0
        }

        if target_difference_count > 0:
            final_avg_metrics["final_average_metrics"]["target_difference"] = total_target_difference / target_difference_count
        
        if mse_count > 0:
            final_avg_metrics["final_average_metrics"]["mse"] = total_mse / mse_count
            final_avg_metrics["final_average_metrics"]["mse_zero_percentage"] = zero_mse_count / mse_count * 100 if mse_count > 0 else 0

            if zero_mse_count > 0:
                final_avg_metrics["final_average_metrics"]["zero_mse_compression_ratio"] = zero_mse_compression_ratio / zero_mse_count
                final_avg_metrics["final_average_metrics"]["zero_mse_size_reduction"] = zero_mse_size_reduction / zero_mse_count

        summary["final_evaluation_metrics"] = final_avg_metrics

    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        logger.info(f"Results saved to: {output_file}")
    except Exception as e:
        logger.error(f"Failed to save results to {output_file}: {e}")
    
    return summary

async def main_async():
    """
    Asynchronous main function
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='Optimize SVG using API and save results')
    parser.add_argument('--input', type=str, required=True, help='Input JSON file path containing origin_svg and opti_ratio fields')
    parser.add_argument('--output', type=str, required=True, help='Output result JSON file path')
    parser.add_argument('--model', type=str, default="deepseekr1", help='Model name to use')
    parser.add_argument('--concurrent', type=int, default=5, help='Maximum concurrent requests')
    
    args = parser.parse_args()

    summary = await process_optimization_batch_from_json(
        args.input,
        args.output,
        args.model,
        args.concurrent
    )

    if "error" in summary:
        print(f"Processing failed: {summary['error']}")
    else:
        print(f"Processing completed, total {summary['total_samples']} samples, {summary['successful_samples']} successful")
        
        # if "average_execution_time" in summary:
        #     print(f"Average execution time: {summary['average_execution_time']:.2f} seconds")
        
        # # Output evaluation metrics
        # if "evaluation_summary" in summary:
        #     eval_summary = summary["evaluation_summary"]
        #     print(f"Average compression ratio: {eval_summary['average_compression']['ratio_percent']:.2f}%")
        #     print(f"Average size reduction: {eval_summary['average_compression']['size_reduction_percent']:.2f}%")
            
        #     if "average_mse" in eval_summary:
        #         print(f"Average MSE: {eval_summary['average_mse']:.4f}")
        #         print(f"Samples with MSE=0: {eval_summary['mse_zero_count']} ({eval_summary['mse_zero_percentage']:.2f}%)")
                
        #         if "zero_mse_samples" in eval_summary:
        #             zero_mse = eval_summary["zero_mse_samples"]
        #             print(f"Average compression ratio for MSE=0 samples: {zero_mse['average_compression_ratio']:.2f}%")
        #             print(f"Average size reduction for MSE=0 samples: {zero_mse['average_size_reduction']:.2f}%")
            
        #     if "average_compression" in eval_summary and "target_difference" in eval_summary["average_compression"]:
        #         print(f"Average target difference: {eval_summary['average_compression']['target_difference']:.2f}%")
        #         print(f"(Total {eval_summary['samples_with_target_ratio']} samples with target compression ratio)")
        
        # # Output final evaluation metrics if they exist
        # if "final_evaluation_metrics" in summary:
        #     final_metrics = summary["final_evaluation_metrics"]["final_average_metrics"]
        #     print("\nFinal average evaluation metrics:")
        #     print(f"  Execution time: {final_metrics['execution_time_seconds']:.2f} seconds")
        #     print(f"  Compression ratio: {final_metrics['compression_ratio_percent']:.2f}%")
        #     print(f"  Size reduction: {final_metrics['size_reduction_percent']:.2f}%")
            
        #     if "mse" in final_metrics:
        #         print(f"  Average MSE: {final_metrics['mse']:.4f}")
        #         print(f"  Percentage of samples with MSE=0: {final_metrics['mse_zero_percentage']:.2f}%")
                
        #         if "zero_mse_compression_ratio" in final_metrics:
        #             print(f"  Average compression ratio for MSE=0 samples: {final_metrics['zero_mse_compression_ratio']:.2f}%")
        #             print(f"  Average size reduction for MSE=0 samples: {final_metrics['zero_mse_size_reduction']:.2f}%")
            
        #     if "target_difference" in final_metrics:
        #         print(f"  Target difference: {final_metrics['target_difference']:.2f}%")
        
        print(f"Detailed results saved to: {args.output}")

def main():
    """
    Command line entry function
    """
    asyncio.run(main_async())

if __name__ == "__main__":
    main()