# ğŸ¨ SVGenius:  Benchmarking LLMs in SVG Understanding, Editing and Generation

![arXiv](https://img.shields.io/badge/arXiv-ViewSpatial%20Bench-red?style=for-the-badge) :page_with_curl:
![Benchmark](https://img.shields.io/badge/Benchmark-ViewSpatial%20Bench-orange?style=for-the-badge) :bar_chart:
![Website](https://img.shields.io/badge/Website-ViewSpatial%20Bench-green?style=for-the-badge) :globe_with_meridians:




## ğŸŒŸ Overview

SVGenius is a pioneering benchmark designed to systematically evaluate the capabilities of Large Language Models in SVG processing. Our benchmark encompasses **24 diverse application domains** and provides comprehensive evaluation across three core dimensions: **Understanding**, **Editing** and **Generation**.

## âœ¨ Key Contributions
Our contributions can be summarized as:

ğŸ” Problem Identification: We identify key limitations in existing SVG evaluation approaches and propose a comprehensive solution
ğŸ¯ Benchmark Innovation: We introduce SVGenius, the first large-scale, complexity-stratified benchmark for SVG processing with real-world data
ğŸ“Š Extensive Evaluation: We provide comprehensive evaluation of 22 models, establishing performance baselines and identifying key factors influencing SVG processing capabilities

## ğŸ“ Repository Structure

```
SVGenius/
â”œâ”€â”€ ğŸ“‚ data/                    # Hierarchical Dataset
â”‚   â”œâ”€â”€ easy/                   # Easy level data
â”‚   â”œâ”€â”€ medium/               # Moderate level data
â”‚   â””â”€â”€ hard/                # Complex level data
â”œâ”€â”€ ğŸ“‚ tasks/                  # Eight task subcategories
â”‚   â”œâ”€â”€ understanding/          # Understanding dimension includes semantic QA and perception QA
â”‚   â”œâ”€â”€ editing/                # Editing dimension includes code optimization, style editing and bug fixing
â”‚   â””â”€â”€ generation/             # Generation dimension includes text-to-svg, multimodel-to-svg and style transferr
â”œâ”€â”€ ğŸ“‚ supplementary/          # Additional materials
â”‚   â””â”€â”€ appendix.pdf           # Appendix includes data construction, tasks, metrics and more details
â””â”€â”€ ğŸ“„ README.md               # This file
```

## ğŸ¯ Task Categories

### ğŸ” Understanding Dimension
- **Perceptual QA**: Visual understanding of SVG elements and layouts
- **Semantic QA**: Comprehension of symbolic meanings and relationships

### âœï¸ Editing Dimension  
- **Bug Fixing**: Identification and correction of SVG code errors
- **Code Optimization**: Performance and efficiency improvements
- **Style Editing**: Visual appearance modifications

### ğŸ¨ Generation Dimension
- **Text-to-SVG Generation**: Creating SVG from textual descriptions
- **Image-to-SVG Generation**: SVG creation from multiple input modalities
- **Style Transfer**: Applying artistic styles to existing SVGs

## ğŸ“Š Benchmark Statistics

| Metric | Count |
|--------|-------|
| Total Samples | 2377 |
| Application Domains | 24 |
| Task Categories | 8 |
| Difficulty Levels | 3 (Easy/Medium/Hard) |
| Evaluated Models | 22 |

ğŸ§ª Model Evaluation
We evaluate a diverse set of models on SVGenius to assess SVG processing capabilities across different architectures, scales, and training paradigms.
Evaluated Models
Our evaluation encompasses:

ğŸ”’ Proprietary Models: GPT-4o, Gemini-2.0-Flash, Claude 3.7-Sonnet
ğŸŒ Open-Source Models: Representative models spanning 1.5B to 72B parameters
  Â·DeepSeek-R1, Qwen2.5/3, Llama-3.2, Mistral-Small
ğŸ¨ SVG-Specialized Systems: Iconshop, StarVector, LLM4SVG

Evaluation Protocol
  Â·Zero-shot Settings: All models evaluated using default configurations
  Â·Complexity Levels: Three difficulty tiers (Easy, Medium, Hard)
  Â·Statistical Robustness: Three independent runs per setting

*Detailed results available in the [supplementary materials](./supplementary/supplementary.pdf).*
